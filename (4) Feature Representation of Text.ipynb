{"cells":[{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"Y-sRUKyfrvsS"},"source":["# Bag of Words Representation"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"3hcItL5yrvsb"},"source":["## How to Generate Bag of Words Representations"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"uWJpnGlnrvsb"},"source":["To make the term-document matrix, we need to find all unique words in our dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"rpVbhVPlrvsb"},"outputs":[],"source":["#finding all unique words\n","all_words = []\n","for item in df_samples_list:\n","    new_abs_words =item['words']\n","    all_words += new_abs_words\n","all_words_unique = list(set(all_words))\n","\n","print('There are ' + str(len(all_words_unique)) + ' unique words in our dataset.')"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"UWqCrmy8rvsb"},"source":["For each chunk of text and each term, mark '1' if it contains the term and mark '0' if it doesn't."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"avUZNq5Crvsb"},"outputs":[],"source":["#making document term matrix\n","word_matrix = {}\n","for word in all_words_unique:\n","    word_vec = []\n","    for item in df_samples_list:\n","        if word in item['words']:\n","            word_vec += [1]\n","        else:\n","            word_vec += [0]\n","    word_matrix[word] = word_vec\n","\n","dc_df = pd.DataFrame(word_matrix)\n","dc_df"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"-GTxsXVurvsb"},"source":["# Vector Space"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"BuTFMKcQrvsb"},"source":["## Vector and Vector Space"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"K0Md9twXrvsb"},"source":["### Introduction to Vectors"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"eiSSp2eNrvsc"},"source":["The word matrix we made above is a good example as the integration of a set of vectors."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"4v5ZAGMIrvsc"},"outputs":[],"source":["dc_df"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"WdbXA11wrvsc"},"source":["Extracting a vector from the whole matrix:"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"Wzxh1tiZrvsc"},"outputs":[],"source":["print(\"The vector for document 0 is: \")\n","pd.DataFrame(dc_df.loc[0,:]).transpose()"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"V4MyKzWVrvsc"},"source":["## How to Caculate Distance between Vectors"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"gn9uNA9Srvsg"},"source":["### How to Calculate Distance/Similarity between Vectors"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"jwztw9Lprvsg"},"source":["Code for Cosine Similarity"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"aB8Bc0Slrvsg"},"outputs":[],"source":["from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n","\n","cos_matrix = cosine_similarity(dc_df)\n","\n","pd.DataFrame(cos_matrix)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"70sxxJP3rvsg"},"source":["Code for Euclidean Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"ktG2qHCnrvsg"},"outputs":[],"source":["#sample code for euclidean distance\n","\n","euc_matrix = euclidean_distances(dc_df)\n","\n","pd.DataFrame(euc_matrix)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"Vhx4c4Onrvsh"},"source":["Finding the document that has the highest similarity to the selected document. Here we choose the eighth document as an example."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"daDGKHqYrvsh"},"outputs":[],"source":["#specify the index of your chosen document\n","chosen_doc = 8\n","scores = sorted(cos_matrix[chosen_doc],reverse=True)\n","score = scores[1]\n","result_doc = list(cos_matrix[chosen_doc]).index(score)\n","# note: you may want to change cos_matrix to euc_matrix and set reverse=False when you are using\n","#       Euclidean distance since the smaller the Euclidean distance is, the similar the two documents are.\n","\n","print('The document that is the most similar with document ' + str(chosen_doc) + ' is ' + 'document ' + str(result_doc) + '.')"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"GtxD3mSHrvsh"},"source":["The results shows that document 7 was the one that is most similar to document 8, which makes sense as they are in the same category. If you investigate the matrix a little bit more, you will see that the documents in the same category have the highest similarity."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"AaP-KcRXrvsh"},"source":["# Term Weighting"]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"hidden":true,"id":"E0I4P7jWrvsh"},"source":["## Code for Different Term Weighting Strategies"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"ymL7nZb4rvsh"},"source":["Then we compute the term frequency(TF) matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"ffMiuL1trvsi"},"outputs":[],"source":["#tf\n","tf_matrix = {}\n","tf_ranking ={}\n","for word in all_words_unique:\n","    word_vec = []\n","    for item in df_samples_list:\n","        if word in item['words']:\n","            word_vec += [item['words'].count(word)/len(item['words'])]\n","        else:\n","            word_vec += [0]\n","    tf_matrix[word] = word_vec\n","\n","pd.DataFrame(tf_matrix)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"KQknQgeMrvsi"},"source":["Showing the term ranking according to the term frequency(TF) score."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"lU0X3ASprvsi"},"outputs":[],"source":["def top_terms(matrix_df, n=10): #input should be a pandas dataframe\n","    output_dict = {}\n","    for index, series in matrix_df.iterrows():\n","        doc_num = 'doc' + str(index)\n","        scores = dict(series)\n","        scores_sorted = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n","        terms = scores_sorted.keys()\n","        terms_topn = list(terms)[:n]\n","        output_dict[doc_num] = terms_topn\n","    output_df = pd.DataFrame(output_dict)\n","    return output_df.transpose()\n","\n","matrix_df = pd.DataFrame(tf_matrix)\n","tf_ranking = top_terms(matrix_df, 10)\n","tf_ranking"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"KSBmi4rOrvsi"},"source":["Computing the inverse document frequency(IDF) matrix."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"RxrVmHYKrvsi"},"outputs":[],"source":["import math\n","\n","#idf\n","idf_matrix = {}\n","for word in word_matrix:\n","    idf_matrix[word] = math.log(len(df_samples_list)/sum(word_matrix[word]))\n","\n","print(\"Inverse Document Frequency Matrix successfully computed!\")"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"GF8fmtCYrvsi"},"source":["Showing the term ranking according to the inverse document frequency(IDF) score."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"PH3R52Utrvsj"},"outputs":[],"source":["idf_ranking = {k: v for k, v in sorted(idf_matrix.items(), key=lambda item: item[1], reverse=True)}\n","idf_ranking"]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"ejzJwryIrvsj"},"outputs":[],"source":["#tfidf\n","tfidf = {}\n","for word in idf_matrix:\n","    idf = idf_matrix[word]\n","    tfidf_vec = tf_matrix[word]\n","    tfidf[word] = [i * idf for i in tfidf_vec]\n","\n","pd.DataFrame(tfidf)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"gJ0phNQwrvsj"},"source":["Finding the terms that have the highest tfidf score and showing the ranking of the terms according to tfidf."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"6TQA8yifrvsj"},"outputs":[],"source":["tfidf_df = pd.DataFrame(tfidf)\n","top_tfidf_terms = top_terms(tfidf_df)\n","\n","top_tfidf_terms"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"z8pw3HnSrvsj"},"source":["These time we can find the most similar document of a chosen document using the tfidf matrix. We don't expect much improvement here since the results calculated from the simple word-document matrix were pretty good as the demo dataset is small and diverse."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"OLMN6Cplrvsj"},"outputs":[],"source":["dc_df = pd.DataFrame(tfidf)\n","cos_matrix = cosine_similarity(dc_df)\n","\n","pd.DataFrame(cos_matrix)"]},{"cell_type":"markdown","metadata":{"hidden":true,"id":"lm1oCBEKrvsk"},"source":["Again, we take document 8 as an example and see the document that is the most similar to document 8."]},{"cell_type":"code","execution_count":null,"metadata":{"hidden":true,"id":"8IBNLJQQrvsk"},"outputs":[],"source":["#specify the index of your chosen document\n","chosen_doc = 8\n","scores = sorted(cos_matrix[chosen_doc],reverse=True)\n","score = scores[1]\n","result_doc = list(cos_matrix[chosen_doc]).index(score)\n","\n","print('The document that is the most similar with document ' + str(chosen_doc) + ' is ' + 'document ' + str(result_doc) + '.')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}