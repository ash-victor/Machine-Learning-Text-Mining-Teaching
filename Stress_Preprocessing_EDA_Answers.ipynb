{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ash-victor/Text-Mining-Teaching/blob/main/Stress_Preprocessing_EDA_Answers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install contractions"
      ],
      "metadata": {
        "id": "7KGGJMygMuWd",
        "outputId": "20322f16-1414-44bc-d862-b8bf5524dd22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.0.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wordcloud"
      ],
      "metadata": {
        "id": "UC3lmsL8NDDg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "gopdcjG-ernZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JD4_6Q45LieT"
      },
      "outputs": [],
      "source": [
        "# Tools to create a data frame(table)\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "# Tools for text preprocessing\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Tools to remove stopwords\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Tools for tokenizing\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "\n",
        "# Tools for lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#Package to help us expand contractions\n",
        "import contractions\n",
        "\n",
        "# Tools for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Tools for doing word frequencies\n",
        "from nltk.util import ngrams\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "#%% Read in a csv file\n",
        "data = pd.read_csv('Stress.csv')\n",
        "\n",
        "#%% Drop any columns you don't want\n",
        "# We will keep all the rows\n",
        "# We will keep columns the first 5 columns\n",
        "\n",
        "data = data.iloc[:, :5]\n",
        "\n",
        "# Drop rows with NaN values\n",
        "data = data.dropna()\n",
        "\n",
        "#%%\n",
        "print(data)\n",
        "\n",
        "#%% Defining a function to remove special characters\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#%% Defining a function to remove stopwords\n",
        "tokenizer = ToktokTokenizer()\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "#stopword_list.remove('no')\n",
        "#stopword_list.remove('not')\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "#%% Defining a function to perform stemming\n",
        "def simple_stemmer(text):\n",
        "    ps = nltk.porter.PorterStemmer()\n",
        "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "#%% Defining a function to perform lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#stopword_list.remove('no')\n",
        "#stopword_list.remove('not')\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    s = \" \"\n",
        "    t_l = []\n",
        "    t_w = nltk.word_tokenize(text)\n",
        "    for w in t_w:\n",
        "        l_w = wordnet_lemmatizer.lemmatize(w, pos=\"v\")\n",
        "        t_l.append(l_w)\n",
        "    text = s.join(t_l)\n",
        "    return text\n",
        "\n",
        "\n",
        "#%% This function takes all the previous preprocessing functions and combines them\n",
        "\n",
        "def normalize_corpus(corpus, text_lower_case=True,\n",
        "                     text_lemmatization=True, special_char_removal=True,\n",
        "                     stopword_removal=True, remove_digits=True):\n",
        "\n",
        "    normalized_corpus = []\n",
        "    # normalize each document in the corpus\n",
        "    for doc in corpus:\n",
        "        # lowercase the text\n",
        "        if text_lower_case:\n",
        "            doc = doc.lower()\n",
        "        doc = contractions.fix(doc)  #Make contractions into full form\n",
        "        # remove extra newlines\n",
        "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
        "        # lemmatize text\n",
        "        if text_lemmatization:\n",
        "            doc = lemmatize_text(doc)\n",
        "        # remove special characters and\\or digits\n",
        "        if special_char_removal:\n",
        "            # insert spaces between special characters to isolate them\n",
        "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
        "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
        "            doc = remove_special_characters(doc, remove_digits=remove_digits)\n",
        "        # remove extra whitespace\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = re.sub('`', '', doc)\n",
        "        # remove stopwords\n",
        "        if stopword_removal:\n",
        "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
        "\n",
        "        normalized_corpus.append(doc)\n",
        "\n",
        "    return normalized_corpus\n",
        "\n",
        "\n",
        "#%% Normalize the text data and add a new column on the csv file of the preprocessed text\n",
        "\n",
        "data['normalized_text'] = normalize_corpus(data['text'])\n",
        "\n",
        "#%% Let's see how our dataset looks like\n",
        "print(data)\n",
        "\n",
        "#%% Save the new csv file\n",
        "data.to_csv('stress_preprocessedd.csv')\n",
        "\n",
        "\n",
        "#%% Exploratory Data Analysis\n",
        "# zero (0) means no stress, one (1) means stress\n",
        "# we will group the subredits based on stress and no stress\n",
        "\n",
        "groups = data.groupby('label').agg({'label':'count'})\n",
        "groups\n",
        "\n",
        "#%% Creating a bar graph\n",
        "\n",
        "bars = plt.bar(range(2),groups.label,color=('b','r'))\n",
        "# Add labels and title\n",
        "plt.xlabel('Labels')\n",
        "plt.ylabel('Number of stressed vs not stressed')\n",
        "plt.title('Bar graph')\n",
        "\n",
        "\n",
        "plt.xticks([0,1])\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#%% Creating bigrams and calculating the word frequency\n",
        "stressed_word=[]\n",
        "for words in data[data.label == 1].normalized_text:\n",
        "    tokens = tokenizer.tokenize(words)\n",
        "    for token in tokens:\n",
        "        stressed_word.append(token)\n",
        "stressed_bigrams_series = (pd.Series(nltk.ngrams(stressed_word, 2)).value_counts())[:20]\n",
        "\n",
        "#%% Creating a plot of the number of bigrams for all the stressed subredits\n",
        "stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Bigrams')\n",
        "plt.ylabel('Bigram')\n",
        "plt.xlabel('# of Occurances')\n",
        "\n",
        "#%% #%% Creating a plot of the number of bigrams for all the non-stressed subredits\n",
        "\n",
        "non_stressed_word=[]\n",
        "for words in data[data.label == 0].normalized_text:\n",
        "    tokens = tokenizer.tokenize(words)\n",
        "    for token in tokens:\n",
        "        non_stressed_word.append(token)\n",
        "non_stressed_bigrams_series = (pd.Series(nltk.ngrams(non_stressed_word, 2)).value_counts())[:20]\n",
        "non_stressed_bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
        "plt.title('20 Most Frequently Occuring Bigrams')\n",
        "plt.ylabel('Bigram')\n",
        "plt.xlabel('# of Occurances')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exploratory Data Analysis: Word Clouds\n",
        "\n",
        "#WordCloud(stopwords=stop_words, collocations=False, max_font_size=55, max_words=25, background_color=\"black\")\n",
        "\n",
        "# Extract the column values as a list\n",
        "column_values = data['normalized_text'].tolist()\n",
        "\n",
        "# Concatenate the column values into a single string\n",
        "concatenated_string = ' '.join(column_values)\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(background_color=\"orange\")\n",
        "\n",
        "# Generate the word cloud from the text\n",
        "wordcloud.generate(concatenated_string)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l-FWNkWJLyUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the corpus and transform the documents into a document-term matrix\n",
        "X = vectorizer.fit_transform(data['normalized_text'])\n",
        "\n",
        "# Get the feature names (unique words) from the vectorizer\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "train_data_features = X.toarray()\n",
        "\n",
        "np.savetxt('matrix.txt', train_data_features)\n",
        "\n"
      ],
      "metadata": {
        "id": "lebgDrBQL94U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}